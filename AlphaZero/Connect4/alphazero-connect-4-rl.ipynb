{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6474927,"sourceType":"datasetVersion","datasetId":3740323}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 使用DeepMind的AlphaZero算法玩四连棋\n\n<img src=\"https://i.imgur.com/ZKo8vb2.png\" \n     align=\"right\" \n     width=\"400\" />\n     \n### 目标\n本笔记本的目标是实现并训练一个AlphaZero智能体来玩四连棋游戏。AlphaZero是AlphaGo Zero算法的通用版本，该算法因仅通过自我对弈在围棋、国际象棋和将棋中达到超人类水平而闻名，且无需任何人类生成的数据。我们旨在探索AlphaZero在四连棋游戏中的适应和优化能力。\n\n### 方法\n1. **游戏引擎**：实现一个四连棋游戏引擎，以便AlphaZero智能体与游戏环境进行交互。\n2. **神经网络**：设计一个深度神经网络模型，用于近似策略（最佳走法）和价值（获胜概率）函数。\n3. **自我对弈**：利用由神经网络引导的蒙特卡洛树搜索生成自我对弈数据。智能体通过自我对弈来探索游戏树。\n4. **训练**：根据自我对弈过程中获得的结果和搜索概率更新神经网络的权重。\n5. **评估**：评估训练后的神经网络识别关键游戏状态（如立即获胜或阻挡）的能力，作为性能的启发式标准。\n\n原始AlphaZero论文可以在此处查看 [这里](https://arxiv.org/abs/1712.01815)。\n\n### 导入和配置","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport math\nimport random\nimport numpy as np\nfrom scipy.signal import convolve2d\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nfrom IPython.display import HTML\nfrom base64 import b64encode\n\n\nconfig_dict = {\n    'device': torch.device('cuda') if torch.cuda.is_available() else 'cpu',\n    'n_filters': 128,              # Number of convolutional filters used in residual blocks\n    'n_res_blocks': 8,             # Number of residual blocks used in network\n    'exploration_constant': 2,     # Exploration constant used in PUCT calculation\n    'temperature': 1.25,           # Selection temperature. A greater temperature is a more uniform distribution\n    'dirichlet_alpha': 1.,         # Alpha parameter for Dirichlet noise. Larger values mean more uniform noise\n    'dirichlet_eps': 0.25,         # Weight of dirichlet noise\n    'learning_rate': 0.001,        # Adam learning rate\n    'training_epochs': 50,         # How many full training epochs\n    'games_per_epoch': 100,        # How many self-played games per epoch\n    'minibatch_size': 128,         # Size of each minibatch used in learning update \n    'n_minibatches': 4,            # How many minibatches to accumulate per learning step\n    'mcts_start_search_iter': 30,  # Number of Monte Carlo tree search iterations initially\n    'mcts_max_search_iter': 150,   # Maximum number of MCTS iterations\n    'mcts_search_increment': 1,    # After each epoch, how much should search iterations be increased by\n    }\n\n# Convert to a struct esque object\nclass Config:\n    def __init__(self, dictionary):\n        for key, value in dictionary.items():\n            setattr(self, key, value)\n\nconfig = Config(config_dict)","metadata":{"execution":{"iopub.status.busy":"2025-03-22T12:51:56.827700Z","iopub.execute_input":"2025-03-22T12:51:56.828005Z","iopub.status.idle":"2025-03-22T12:52:00.291486Z","shell.execute_reply.started":"2025-03-22T12:51:56.827951Z","shell.execute_reply":"2025-03-22T12:52:00.290527Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Connect 4 Game Logic\n这个类 `Connect4` 作为四连棋的游戏引擎。它定义了游戏规则、棋盘配置和状态转换。棋盘被建模为一个6x7的网格，并且该类提供了多种方法用于处理与游戏相关的任务。\n\n为了高效计算，大量使用了数组数据结构。\n","metadata":{}},{"cell_type":"code","source":"class Connect4:\n    \"Connect 4 game engine, containing methods for game-related tasks.\"\n    def __init__(self):\n        self.rows = 6\n        self.cols = 7\n\n    def get_next_state(self, state, action, to_play=1):\n        \"Play an action in a given state and return the resulting board.\"\n        # Pre-condition checks\n        assert self.evaluate(state) == 0\n        assert np.sum(abs(state)) != self.rows * self.cols\n        assert action in self.get_valid_actions(state)\n        \n        # Identify next empty row in column\n        row = np.where(state[:, action] == 0)[0][-1]\n        \n        # Apply action\n        new_state = state.copy()\n        new_state[row, action] = to_play\n        return new_state\n\n    def get_valid_actions(self, state):\n        \"Return a numpy array containing the indices of valid actions.\"\n        # If game over, no valid moves\n        if self.evaluate(state) != 0:\n            return np.array([])\n        \n        # Identify valid columns to play\n        cols = np.sum(np.abs(state), axis=0)\n        return np.where((cols // self.rows) == 0)[0]\n\n    def evaluate(self, state):\n        \"Evaluate the current position. Returns 1 for player 1 win, -1 for player 2 and 0 otherwise.\"\n        # Kernels for checking win conditions\n        kernel = np.ones((1, 4), dtype=int)\n        \n        # Horizontal and vertical checks\n        horizontal_check = convolve2d(state, kernel, mode='valid')\n        vertical_check = convolve2d(state, kernel.T, mode='valid')\n\n        # Diagonal checks\n        diagonal_kernel = np.eye(4, dtype=int)\n        main_diagonal_check = convolve2d(state, diagonal_kernel, mode='valid')\n        anti_diagonal_check = convolve2d(state, np.fliplr(diagonal_kernel), mode='valid')\n        \n        # Check for winner\n        if any(cond.any() for cond in [horizontal_check == 4, vertical_check == 4, main_diagonal_check == 4, anti_diagonal_check == 4]):\n            return 1\n        elif any(cond.any() for cond in [horizontal_check == -4, vertical_check == -4, main_diagonal_check == -4, anti_diagonal_check == -4]):\n            return -1\n\n        # No winner\n        return 0  \n\n    def step(self, state, action, to_play=1):\n        \"Play an action in a given state. Return the next_state, reward and done flag.\"\n        # Get new state and reward\n        next_state = self.get_next_state(state, action, to_play)\n        reward = self.evaluate(next_state)\n        \n        # Check for game termination\n        done = True if reward != 0 or np.sum(abs(next_state)) >= (self.rows * self.cols - 1) else False\n        return next_state, reward, done\n\n    def encode_state(self, state):\n        \"Convert state to tensor with 3 channels.\"\n        encoded_state = np.stack((state == 1, state == 0, state == -1)).astype(np.float32)\n        if len(state.shape) == 3:\n            encoded_state = np.swapaxes(encoded_state, 0, 1)\n        return encoded_state\n\n    def reset(self):\n        \"Reset the board.\"\n        return np.zeros([self.rows, self.cols], dtype=np.int8)","metadata":{"execution":{"iopub.status.busy":"2025-03-22T13:17:27.801262Z","iopub.execute_input":"2025-03-22T13:17:27.801590Z","iopub.status.idle":"2025-03-22T13:17:27.814227Z","shell.execute_reply.started":"2025-03-22T13:17:27.801560Z","shell.execute_reply":"2025-03-22T13:17:27.813273Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### The Residual Neural Network\nThe code defines the architecture for a residual neural network (ResNet) that serves as the core of the AlphaZero algorithm applied to Connect 4. ResNets are known for their ability to learn from the addition of residual (or \"skip\") connections, mitigating the vanishing gradient problem in deep networks [He et al. 2015](https://arxiv.org/abs/1512.03385).\n\nIn the context of AlphaZero, this ResNet is designed with two distinct heads branching from the common convolutional base:\n\n1. **Policy head**: Outputs a probability distribution over possible actions, guiding the agent on which column to drop the disc in during gameplay.\n2. **Value Head**: Produces a scalar value within the range [-1, 1] that approximates the value of the current board state, aiding in evaluating the board's win-lose potential.\n\nThe network accepts a three-channeled representation of the board. These channels capture three aspects of the game state: the positions of the player's discs, the opponent's discs, and empty space, making it easier for the network to discriminate and process the board information.\n\nResNets are particularly effective for board games like Connect 4 because they're good at understanding the layout and spatial relationships on the board. The residual connections help the network better pass along this spatial information through its layers.","metadata":{}},{"cell_type":"code","source":"class ResNet(nn.Module):\n    \"Complete residual neural network model.\"\n    def __init__(self, game, config):\n        super().__init__()\n\n        # Board dimensions\n        self.board_size = (game.rows, game.cols)\n        n_actions = game.cols  # Number of columns represent possible actions\n        n_filters = config.n_filters\n        \n        self.base = ConvBase(config)  # Base layers\n\n        # Policy head for choosing actions\n        self.policy_head = nn.Sequential(\n            nn.Conv2d(n_filters, n_filters//4, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n_filters//4),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(n_filters//4 * self.board_size[0] * self.board_size[1], n_actions)\n        )\n\n        # Value head for evaluating board states\n        self.value_head = nn.Sequential(\n            nn.Conv2d(n_filters, n_filters//32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n_filters//32),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(n_filters//32 * self.board_size[0] * self.board_size[1], 1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = self.base(x) \n        x_value = self.value_head(x)\n        x_policy = self.policy_head(x)\n        return x_value, x_policy\n\nclass ConvBase(nn.Module):\n    \"Convolutional base for the network.\"\n    def __init__(self, config):\n        super().__init__()\n        \n        n_filters = config.n_filters\n        n_res_blocks = config.n_res_blocks\n\n        # Initial convolutional layer\n        self.conv = nn.Sequential(\n            nn.Conv2d(3, n_filters, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n_filters),\n            nn.ReLU()\n        )\n\n        # List of residual blocks\n        self.res_blocks = nn.ModuleList(\n            [ResidualBlock(n_filters) for _ in range(n_res_blocks)]\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        for block in self.res_blocks:\n            x = block(x)\n        return x\n\nclass ResidualBlock(nn.Module):\n    \"Residual block, the backbone of a ResNet.\"\n    def __init__(self, n_filters):\n        super().__init__()\n\n        # Two convolutional layers, both with batch normalization\n        self.conv_1 = nn.Conv2d(n_filters, n_filters, kernel_size=3, padding=1)\n        self.batch_norm_1 = nn.BatchNorm2d(n_filters)\n        self.conv_2 = nn.Conv2d(n_filters, n_filters, kernel_size=3, padding=1)\n        self.batch_norm_2 = nn.BatchNorm2d(n_filters)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        # Pass x through layers and add skip connection\n        output = self.relu(self.batch_norm_1(self.conv_1(x)))\n        output = self.batch_norm_2(self.conv_2(output))\n        return self.relu(output + x)","metadata":{"execution":{"iopub.status.busy":"2025-03-22T13:19:34.538873Z","iopub.execute_input":"2025-03-22T13:19:34.539223Z","iopub.status.idle":"2025-03-22T13:19:34.550313Z","shell.execute_reply.started":"2025-03-22T13:19:34.539196Z","shell.execute_reply":"2025-03-22T13:19:34.549389Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### AlphaZero 中的蒙特卡洛树搜索\n蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）是一种广泛应用于棋盘游戏中的决策算法。传统的 MCTS 使用随机模拟（rollouts）来估计每个状态的价值，这种方法在样本量不足时计算成本高且通常不准确。\n\n#### 传统 MCTS 的局限性\n- **近似不准确**：传统方法通过随机模拟来近似状态的价值，这种近似在复杂的游戏空间中可能效果较差。\n- **计算成本高**：为了获得准确的近似值，所需的模拟次数可能非常多，导致计算成本极高。\n\n#### AlphaZero 的改进\n- **神经网络评估**：AlphaZero 不再依赖随机模拟，而是使用神经网络来近似状态价值和策略（动作概率），从而使搜索更加有方向性且高效。\n- **Dirichlet 噪声**：AlphaZero 在根节点的先验概率中添加 Dirichlet 噪声，以鼓励探索访问较少的节点。\n- **PUCT 分数**：传统的 MCTS 使用 UCB（上置信界）进行树遍历，而 AlphaZero 使用改进版的 PUCT（多项式上置信树），更有效地平衡探索与利用。\n\n通过解决传统 MCTS 的局限性并整合神经网络近似函数，AlphaZero 显著提高了树搜索算法的效率和准确性。","metadata":{}},{"cell_type":"code","source":"class MCTS:\n    def __init__(self, network, game, config):\n        \"Initialize Monte Carlo Tree Search with a given neural network, game instance, and configuration.\"\n        self.network = network\n        self.game = game\n        self.config = config\n\n    def search(self, state, total_iterations, temperature=None):\n        \"Performs a search for the desired number of iterations, returns an action and the tree root.\"\n        # Create the root\n        root = Node(None, state, 1, self.game, self.config)\n\n        # Expand the root, adding noise to each action\n        # Get valid actions\n        valid_actions = self.game.get_valid_actions(state)\n        state_tensor = torch.tensor(self.game.encode_state(state), dtype=torch.float).unsqueeze(0).to(self.config.device)\n        with torch.no_grad():\n            self.network.eval()\n            value, logits = self.network(state_tensor)\n\n        # Get action probabilities\n        action_probs = F.softmax(logits.view(self.game.cols), dim=0).cpu().numpy()\n\n        # Calculate and add dirichlet noise\n        noise = np.random.dirichlet([self.config.dirichlet_alpha] * self.game.cols)\n        action_probs = ((1 - self.config.dirichlet_eps) * action_probs) + self.config.dirichlet_eps * noise\n\n        # Mask unavailable actions\n        mask = np.full(self.game.cols, False)\n        mask[valid_actions] = True\n        action_probs = action_probs[mask]\n\n        # Softmax\n        action_probs /= np.sum(action_probs)\n\n        # Create a child for each possible action\n        for action, prob in zip(valid_actions, action_probs):\n            child_state = -self.game.get_next_state(state, action)\n            root.children[action] = Node(root, child_state, -1, self.game, self.config)\n            root.children[action].prob = prob\n\n        # Since we're not backpropagating, manually increase visits\n        root.n_visits = 1\n        # Set value as neural network prediction also as it will slightly improve the accuracy of the value target later\n        root.total_score = value.item()\n\n        # Begin search\n        for _ in range(total_iterations):\n            current_node = root\n\n            # Phase 1: Selection\n            # While not currently on a leaf node, select a new node using PUCT score\n            while not current_node.is_leaf():\n                current_node = current_node.select_child()\n\n            # Phase 2: Expansion\n            # When a leaf node is reached and it's not terminal; expand it\n            if not current_node.is_terminal():\n                current_node.expand()\n                # Convert node state to tensor and pass through network\n                state_tensor = torch.tensor(self.game.encode_state(current_node.state), dtype=torch.float).unsqueeze(0).to(self.config.device)\n                with torch.no_grad():\n                    self.network.eval()\n                    value, logits = self.network(state_tensor)\n                    value = value.item()\n\n                # Mask invalid actions, then calculate masked action probs\n                mask = np.full(self.game.cols, False)\n                mask[valid_actions] = True\n                action_probs = F.softmax(logits.view(self.game.cols)[mask], dim=0).cpu().numpy()\n                for child, prob in zip(current_node.children.values(), action_probs):\n                    child.prob = prob\n            # If node is terminal, get the value of it from game instance\n            else:\n                value = self.game.evaluate(current_node.state)\n\n            # Phase 3: Backpropagation\n            # Backpropagate the value of the leaf to the root\n            current_node.backpropagate(value)\n        \n        # Select action with specified temperature\n        if temperature == None:\n            temperature = self.config.temperature\n        return self.select_action(root, temperature), root\n\n    def select_action(self, root, temperature=None):\n        \"Select an action from the root based on visit counts, adjusted by temperature, 0 temp for greedy.\"\n        if temperature == None:\n            temperature = self.config.temperature\n        action_counts = {key: val.n_visits for key, val in root.children.items()}\n        if temperature == 0:\n            return max(action_counts, key=action_counts.get)\n        elif temperature == np.inf:\n            return np.random.choice(list(action_counts.keys()))\n        else:\n            distribution = np.array([*action_counts.values()]) ** (1 / temperature)\n            return np.random.choice([*action_counts.keys()], p=distribution/sum(distribution))\n\nclass Node:\n    def __init__(self, parent, state, to_play, game, config):\n        \"Represents a node in the MCTS, holding the game state and statistics for MCTS to operate.\"\n        self.parent = parent\n        self.state = state\n        self.to_play = to_play\n        self.config = config\n        self.game = game\n\n        self.prob = 0\n        self.children = {}\n        self.n_visits = 0\n        self.total_score = 0\n\n    def expand(self):\n        \"Create child nodes for all valid actions. If state is terminal, evaluate and set the node's value.\"\n        # Get valid actions\n        valid_actions = self.game.get_valid_actions(self.state)\n\n        # If there are no valid actions, state is terminal, so get value using game instance\n        if len(valid_actions) == 0:\n            self.total_score = self.game.evaluate(self.state)\n            return\n\n        # Create a child for each possible action\n        for action in zip(valid_actions):\n            # Make move, then flip board to perspective of next player\n            child_state = -self.game.get_next_state(self.state, action)\n            self.children[action] = Node(self, child_state, -self.to_play, self.game, self.config)\n\n    def select_child(self):\n        \"Select the child node with the highest PUCT score.\"\n        best_puct = -np.inf\n        best_child = None\n        for child in self.children.values():\n            puct = self.calculate_puct(child)\n            if puct > best_puct:\n                best_puct = puct\n                best_child = child\n        return best_child\n\n    def calculate_puct(self, child):\n        \"Calculate the PUCT score for a given child node.\"\n        # Scale Q(s,a) so it's between 0 and 1 so it's comparable to a probability\n        # Using 1 - Q(s,a) because it's from the perspectve of the child – the opposite of the parent\n        exploitation_term = 1 - (child.get_value() + 1) / 2\n        exploration_term = child.prob * math.sqrt(self.n_visits) / (child.n_visits + 1)\n        return exploitation_term + self.config.exploration_constant * exploration_term\n\n    def backpropagate(self, value):\n        \"Update the current node and its ancestors with the given value.\"\n        self.total_score += value\n        self.n_visits += 1\n        if self.parent is not None:\n            # Backpropagate the negative value so it switches each level\n            self.parent.backpropagate(-value)\n\n    def is_leaf(self):\n        \"Check if the node is a leaf (no children).\"\n        return len(self.children) == 0\n\n    def is_terminal(self):\n        \"Check if the node represents a terminal state.\"\n        return (self.n_visits != 0) and (len(self.children) == 0)\n\n    def get_value(self):\n        \"Calculate the average value of this node.\"\n        if self.n_visits == 0:\n            return 0\n        return self.total_score / self.n_visits\n    \n    def __str__(self):\n        \"Return a string containing the node's relevant information for debugging purposes.\"\n        return (f\"State:\\n{self.state}\\nProb: {self.prob}\\nTo play: {self.to_play}\" +\n                f\"\\nNumber of children: {len(self.children)}\\nNumber of visits: {self.n_visits}\" +\n                f\"\\nTotal score: {self.total_score}\")","metadata":{"execution":{"iopub.status.busy":"2025-03-22T13:44:26.594029Z","iopub.execute_input":"2025-03-22T13:44:26.594536Z","iopub.status.idle":"2025-03-22T13:44:26.628536Z","shell.execute_reply.started":"2025-03-22T13:44:26.594496Z","shell.execute_reply":"2025-03-22T13:44:26.627744Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### AlphaZero 算法\n\n`AlphaZero` 类封装了 AlphaZero 算法的核心部分。它将蒙特卡洛树搜索（MCTS）与残差网络（ResNet）结合，实现自主对弈和学习。\n\n#### 组件\n- **初始化**：设置神经网络、MCTS，并预分配 GPU 内存以用于训练。\n- **内存管理**：利用预分配的 GPU 内存高效存储训练数据。\n- **训练循环**：在指定的训练轮次（epochs）中协调自我对弈和学习过程。\n- **自我对弈**：执行一局完整的对弈，并将游戏状态、价值和策略存储到内存中。\n- **学习步骤**：执行多次小批量学习步骤，更新神经网络参数。","metadata":{}},{"cell_type":"code","source":"class AlphaZero:\n    def __init__(self, game, config, verbose=True):\n        self.network = ResNet(game, config).to(config.device)\n        self.mcts = MCTS(self.network, game, config)\n        self.game = game\n        self.config = config\n\n        # Losses and optimizer\n        self.loss_cross_entropy = nn.CrossEntropyLoss()\n        self.loss_mse = nn.MSELoss()\n        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=config.learning_rate, weight_decay=0.0001)\n\n        # Pre-allocate memory on GPU\n        state_shape = game.encode_state(game.reset()).shape\n        self.max_memory = config.minibatch_size * config.n_minibatches\n        self.state_memory = torch.zeros(self.max_memory, *state_shape).to(config.device)\n        self.value_memory = torch.zeros(self.max_memory, 1).to(config.device)\n        self.policy_memory = torch.zeros(self.max_memory, game.cols).to(config.device)\n        self.current_memory_index = 0\n        self.memory_full = False\n\n        # MCTS search iterations\n        self.search_iterations = config.mcts_start_search_iter\n        \n        # Logging\n        self.verbose = verbose\n        self.total_games = 0\n\n    def train(self, training_epochs):\n        \"Train the AlphaZero agent for a specified number of training epochs.\"\n        # For each training epoch\n        for _ in range(training_epochs):\n\n            # Play specified number of games\n            for _ in range(self.config.games_per_epoch):\n                self.self_play()\n            \n            # At the end of each epoch, increase the number of MCTS search iterations\n            self.search_iterations = min(self.config.mcts_max_search_iter, self.search_iterations + self.config.mcts_search_increment)\n\n    def self_play(self):\n        \"Perform one episode of self-play.\"\n        state = self.game.reset()\n        done = False\n        while not done:\n            # Search for a move\n            action, root = self.mcts.search(state, self.search_iterations)\n\n            # Value target is the value of the MCTS root node\n            value = root.get_value()\n\n            # Visit counts used to compute policy target\n            visits = np.zeros(self.game.cols)\n            for child_action, child in root.children.items():\n                visits[child_action] = child.n_visits\n            # Softmax so distribution sums to 1\n            visits /= np.sum(visits)\n\n            # Append state + value & policy targets to memory\n            self.append_to_memory(state, value, visits)\n\n            # If memory is full, perform a learning step\n            if self.memory_full:\n                self.learn()\n\n            # Perform action in game\n            state, _, done = self.game.step(state, action)\n\n            # Flip the board\n            state = -state\n\n        # Increment total games played\n        self.total_games += 1\n\n        # Logging if verbose\n        if self.verbose:\n            print(\"\\rTotal Games:\", self.total_games, \"Items in Memory:\", self.current_memory_index, \"Search Iterations:\", self.search_iterations, end=\"\")\n\n    def append_to_memory(self, state, value, visits):\n        \"\"\"\n        Append state and MCTS results to memory buffers.\n        Args:\n            state (array-like): Current game state.\n            value (float): MCTS value for the game state.\n            visits (array-like): MCTS visit counts for available moves.\n        \"\"\"\n        # Calculate the encoded states\n        encoded_state = np.array(self.game.encode_state(state))\n        encoded_state_augmented = np.array(self.game.encode_state(state[:, ::-1]))\n\n        # Stack states and visits\n        states_stack = np.stack((encoded_state, encoded_state_augmented), axis=0)\n        visits_stack = np.stack((visits, visits[::-1]), axis=0)\n\n        # Convert the stacks to tensors\n        state_tensor = torch.tensor(states_stack, dtype=torch.float).to(self.config.device)\n        visits_tensor = torch.tensor(visits_stack, dtype=torch.float).to(self.config.device)\n        value_tensor = torch.tensor(np.array([value, value]), dtype=torch.float).to(self.config.device).unsqueeze(1)\n\n        # Store in pre-allocated GPU memory\n        self.state_memory[self.current_memory_index:self.current_memory_index + 2] = state_tensor\n        self.value_memory[self.current_memory_index:self.current_memory_index + 2] = value_tensor\n        self.policy_memory[self.current_memory_index:self.current_memory_index + 2] = visits_tensor\n\n        # Increment index, handle overflow\n        self.current_memory_index = (self.current_memory_index + 2) % self.max_memory\n\n        # Set memory filled flag to True if memory is full\n        if (self.current_memory_index == 0) or (self.current_memory_index == 1):\n            self.memory_full = True\n\n\n    def learn(self):\n        \"Update the neural network by extracting minibatches from memory and performing one step of optimization for each one.\"\n        self.network.train()\n\n        # Create a randomly shuffled list of batch indices\n        batch_indices = np.arange(self.max_memory)\n        np.random.shuffle(batch_indices)\n\n        for batch_index in range(self.config.n_minibatches):\n            # Get minibatch indices\n            start = batch_index * self.config.minibatch_size\n            end = start + self.config.minibatch_size\n            mb_indices = batch_indices[start:end]\n\n            # Slice memory tensors\n            mb_states = self.state_memory[mb_indices]\n            mb_value_targets = self.value_memory[mb_indices]\n            mb_policy_targets = self.policy_memory[mb_indices]\n\n            # Network predictions\n            value_preds, policy_logits = self.network(mb_states)\n\n            # Loss calculation\n            policy_loss = self.loss_cross_entropy(policy_logits, mb_policy_targets)\n            value_loss = self.loss_mse(value_preds.view(-1), mb_value_targets.view(-1))\n            loss = policy_loss + value_loss\n\n            # Backward pass\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n        self.memory_full = False\n        self.network.eval()","metadata":{"execution":{"iopub.status.busy":"2025-03-22T13:57:49.756038Z","iopub.execute_input":"2025-03-22T13:57:49.756315Z","iopub.status.idle":"2025-03-22T13:57:49.772464Z","shell.execute_reply.started":"2025-03-22T13:57:49.756295Z","shell.execute_reply":"2025-03-22T13:57:49.771560Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### 评估器\n`Evaluator` 类的目的是评估神经网络在做出明显获胜动作（例如直接获胜或阻止对手获胜）方面的能力。它与现有的 AlphaZero 实例集成，继承其网络、游戏逻辑和配置。虽然这不是评估高水平对弈的有效方法，但该类有助于确认 AlphaZero 智能体是否在学习。\n\n#### 组件\n- **初始化**：接收一个 AlphaZero 实例，并准备一组示例状态和动作用于评估。\n- **动作选择**：使用简单的游戏逻辑找到一个动作，要么立即获胜，要么阻止对手获胜。\n- **示例生成**：构建用于获胜和阻止对手获胜的场景示例集，随后对其进行编码以进行评估。\n- **评估**：计算神经网络在生成的示例上的预测准确率。","metadata":{}},{"cell_type":"code","source":"class Evaluator:\n    \"Class to evaluate the policy network's performance on simple moves.\"\n    def __init__(self, alphazero, num_examples=500, verbose=True):\n        self.network = alphazero.network\n        self.game = alphazero.game\n        self.config = alphazero.config\n        self.accuracies = []\n        self.num_examples = num_examples\n        self.verbose = verbose\n\n        # Generate and prepare example states and actions for evaluation\n        self.generate_examples()\n\n    def select_action(self, state):\n        \"Select an action based on the given state, will choose a winning or blocking moves.\"\n        valid_actions = self.game.get_valid_actions(state)\n        \n        # Check for a winning move\n        for action in valid_actions:\n            next_state, reward, _ = self.game.step(state, action)\n            if reward == 1:\n                return action\n\n        # Check for a blocking move\n        flipped_state = -state\n        for action in valid_actions:\n            next_state, reward, _ = self.game.step(flipped_state, action)\n            if reward == 1:\n                return action\n\n        # Default to random action if no winning or blocking move\n        return random.choice(valid_actions)\n\n    def generate_examples(self):\n        \"Generate and prepare example states and actions for evaluation.\"\n        winning_examples = self.generate_examples_for_condition('win')\n        blocking_examples = self.generate_examples_for_condition('block')\n\n        # Prepare states and actions for evaluation\n        winning_example_states, winning_example_actions = zip(*winning_examples)\n        blocking_example_states, blocking_example_actions = zip(*blocking_examples)\n\n        target_states = np.concatenate([winning_example_states, blocking_example_states], axis=0)\n        target_actions = np.concatenate([winning_example_actions, blocking_example_actions], axis=0)\n\n        encoded_states = [self.game.encode_state(state) for state in target_states]\n        self.X_target = torch.tensor(np.stack(encoded_states, axis=0), dtype=torch.float).to(self.config.device)\n        self.y_target = torch.tensor(target_actions, dtype=torch.long).to(self.config.device)\n\n    def generate_examples_for_condition(self, condition):\n        \"Generate examples based on either 'win' or 'block' conditions.\"\n        examples = []\n        while len(examples) < self.num_examples:\n            state = self.game.reset()\n            while True:\n                action = self.select_action(state)\n                next_state, reward, done = self.game.step(state, action, to_play=1)\n                \n                if condition == 'win' and reward == 1:\n                    examples.append((state, action))\n                    break\n                \n                if done:\n                    break\n                \n                state = next_state\n\n                # Flipping the board for opponent's perspective\n                action = self.select_action(-state)\n                next_state, reward, done = self.game.step(state, action, to_play=-1)\n                \n                if condition == 'block' and reward == -1:\n                    examples.append((-state, action))\n                    break\n                \n                if done:\n                    break\n                \n                state = next_state\n        return examples\n\n    def evaluate(self):\n        \"Evaluate the policy network's accuracy and append it to self.accuracies.\"\n        with torch.no_grad():\n            self.network.eval()\n            _, logits = self.network(self.X_target)\n            pred_actions = logits.argmax(dim=1)\n            accuracy = (pred_actions == self.y_target).float().mean().item()\n        \n        self.accuracies.append(accuracy)\n        if self.verbose:\n            print(f\"Initial Evaluation Accuracy: {100 * accuracy:.1f}%\")","metadata":{"execution":{"iopub.status.busy":"2025-03-22T14:04:06.936105Z","iopub.execute_input":"2025-03-22T14:04:06.936432Z","iopub.status.idle":"2025-03-22T14:04:06.948557Z","shell.execute_reply.started":"2025-03-22T14:04:06.936407Z","shell.execute_reply":"2025-03-22T14:04:06.947578Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### 训练与评估循环\n这段代码负责组织和运行训练与评估过程。智能体首先训练一个周期（epoch），然后进行评估。","metadata":{}},{"cell_type":"code","source":"game = Connect4()\nalphazero = AlphaZero(game, config)\nevaluator = Evaluator(alphazero)\n\n# Evaluate pre training\nevaluator.evaluate()\n\n# Main training/eval loop\nfor _ in range(config.training_epochs):\n    alphazero.train(1)\n    evaluator.evaluate()\n\n# Save trained weights\ntorch.save(alphazero.network.state_dict(), 'alphazero-network-weights.pth')","metadata":{"execution":{"iopub.status.busy":"2025-03-22T14:04:32.837591Z","iopub.execute_input":"2025-03-22T14:04:32.838137Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Initial Evaluation Accuracy: 20.3%\nTotal Games: 100 Items in Memory: 306 Search Iterations: 30Initial Evaluation Accuracy: 24.7%\nTotal Games: 200 Items in Memory: 94 Search Iterations: 31Initial Evaluation Accuracy: 25.4%\nTotal Games: 300 Items in Memory: 360 Search Iterations: 32Initial Evaluation Accuracy: 23.7%\nTotal Games: 400 Items in Memory: 10 Search Iterations: 33Initial Evaluation Accuracy: 22.0%\nTotal Games: 500 Items in Memory: 196 Search Iterations: 34Initial Evaluation Accuracy: 18.3%\nTotal Games: 600 Items in Memory: 40 Search Iterations: 35Initial Evaluation Accuracy: 20.8%\nTotal Games: 700 Items in Memory: 178 Search Iterations: 36Initial Evaluation Accuracy: 19.6%\nTotal Games: 800 Items in Memory: 336 Search Iterations: 37Initial Evaluation Accuracy: 20.7%\nTotal Games: 900 Items in Memory: 410 Search Iterations: 38Initial Evaluation Accuracy: 29.0%\nTotal Games: 1000 Items in Memory: 132 Search Iterations: 39Initial Evaluation Accuracy: 28.4%\nTotal Games: 1100 Items in Memory: 58 Search Iterations: 40Initial Evaluation Accuracy: 30.3%\nTotal Games: 1200 Items in Memory: 476 Search Iterations: 41Initial Evaluation Accuracy: 18.7%\nTotal Games: 1300 Items in Memory: 168 Search Iterations: 42Initial Evaluation Accuracy: 29.2%\nTotal Games: 1400 Items in Memory: 242 Search Iterations: 43Initial Evaluation Accuracy: 28.4%\nTotal Games: 1500 Items in Memory: 96 Search Iterations: 44Initial Evaluation Accuracy: 48.7%\nTotal Games: 1600 Items in Memory: 378 Search Iterations: 45Initial Evaluation Accuracy: 49.8%\nTotal Games: 1700 Items in Memory: 8 Search Iterations: 46Initial Evaluation Accuracy: 33.7%\nTotal Games: 1800 Items in Memory: 66 Search Iterations: 47Initial Evaluation Accuracy: 36.8%\nTotal Games: 1900 Items in Memory: 352 Search Iterations: 48Initial Evaluation Accuracy: 59.1%\nTotal Games: 2000 Items in Memory: 344 Search Iterations: 49Initial Evaluation Accuracy: 55.9%\nTotal Games: 2100 Items in Memory: 328 Search Iterations: 50Initial Evaluation Accuracy: 63.0%\nTotal Games: 2200 Items in Memory: 354 Search Iterations: 51Initial Evaluation Accuracy: 54.2%\nTotal Games: 2300 Items in Memory: 454 Search Iterations: 52Initial Evaluation Accuracy: 49.2%\nTotal Games: 2400 Items in Memory: 282 Search Iterations: 53Initial Evaluation Accuracy: 63.1%\nTotal Games: 2500 Items in Memory: 108 Search Iterations: 54Initial Evaluation Accuracy: 63.5%\nTotal Games: 2600 Items in Memory: 382 Search Iterations: 55Initial Evaluation Accuracy: 67.9%\nTotal Games: 2700 Items in Memory: 116 Search Iterations: 56Initial Evaluation Accuracy: 60.9%\nTotal Games: 2800 Items in Memory: 344 Search Iterations: 57Initial Evaluation Accuracy: 65.7%\nTotal Games: 2900 Items in Memory: 258 Search Iterations: 58Initial Evaluation Accuracy: 59.4%\nTotal Games: 3000 Items in Memory: 26 Search Iterations: 59Initial Evaluation Accuracy: 69.7%\nTotal Games: 3100 Items in Memory: 102 Search Iterations: 60Initial Evaluation Accuracy: 70.1%\nTotal Games: 3200 Items in Memory: 268 Search Iterations: 61Initial Evaluation Accuracy: 72.8%\nTotal Games: 3300 Items in Memory: 424 Search Iterations: 62Initial Evaluation Accuracy: 75.2%\nTotal Games: 3400 Items in Memory: 50 Search Iterations: 63Initial Evaluation Accuracy: 64.3%\nTotal Games: 3500 Items in Memory: 60 Search Iterations: 64Initial Evaluation Accuracy: 75.6%\nTotal Games: 3600 Items in Memory: 34 Search Iterations: 65Initial Evaluation Accuracy: 76.0%\nTotal Games: 3700 Items in Memory: 56 Search Iterations: 66Initial Evaluation Accuracy: 73.9%\nTotal Games: 3800 Items in Memory: 466 Search Iterations: 67Initial Evaluation Accuracy: 78.6%\nTotal Games: 3900 Items in Memory: 418 Search Iterations: 68Initial Evaluation Accuracy: 74.9%\nTotal Games: 3938 Items in Memory: 288 Search Iterations: 69","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"### 训练期间策略网络性能的可视化\n该图表展示了策略网络（policy network）在训练过程中性能的提升情况。需要注意的是，策略网络仅能看到当前可用的走法，而不会使用任何树搜索（tree search）来预测未来的走法。这意味着图中显示的性能与实际对弈中使用树搜索时的性能相比，是一个严重低估的结果。尽管如此，它仍然是一个有用的指标，用于跟踪策略网络在训练过程中的改进情况。","metadata":{}},{"cell_type":"code","source":"# Plot data\nx_values = np.linspace(0, 101 * len(evaluator.accuracies), len(evaluator.accuracies))\ny_values = [acc * 100 for acc in evaluator.accuracies]\n\n# Create plot\nplt.figure(figsize=(10, 6))\nplt.plot(x_values, y_values, linewidth=2, marker='o', markersize=4, linestyle='-', color='#636EFA')\n\n# Formatting\nplt.xlabel('\\nNumber of Games', fontsize=16)\nplt.ylabel('Policy Evaluation Accuracy (%)', fontsize=16)\nplt.title('Policy Evaluation\\n', fontsize=24)\nplt.grid(True, linestyle='--', linewidth=0.5, color='gray')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T23:16:05.303272Z","iopub.execute_input":"2023-09-14T23:16:05.303649Z","iopub.status.idle":"2023-09-14T23:16:05.586903Z","shell.execute_reply.started":"2023-09-14T23:16:05.303623Z","shell.execute_reply":"2023-09-14T23:16:05.586193Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Play Against AlphaZero\nThe following code allows for a person to play vs AlphaZero. The number of search iterations can be adjusted as desired. A search depth of 0 will only utilize the policy network in decision making.\n\n#### Load Pre-Trained Weights\nIn the case that you'd prefer not train the model yourself (I include myself in this group), you can load the neural network weights from a model I trained for about 24 hours on around 13,000 games. I will make these weights available on GitHub in the near future. Comment out these lines if you trained the model yourself.","metadata":{}},{"cell_type":"code","source":"file_path = \"/kaggle/input/alphazero-model-and-video/alphazero-network-weights-130-epochs.pth\"\npre_trained_weights = torch.load(file_path, map_location=config.device)\n\nalphazero.network.load_state_dict(pre_trained_weights)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T23:19:50.357487Z","iopub.execute_input":"2023-09-14T23:19:50.357822Z","iopub.status.idle":"2023-09-14T23:19:50.494916Z","shell.execute_reply.started":"2023-09-14T23:19:50.357793Z","shell.execute_reply":"2023-09-14T23:19:50.494249Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### AlphaZero Agent\nA simple class that intergrates with AlphaZero for move selection using Monte Carlo tree search to a specified depth.","metadata":{}},{"cell_type":"code","source":"class AlphaZeroAgent:\n    def __init__(self, alphazero):\n        self.alphazero = alphazero\n        self.alphazero.network.eval()\n        \n        # Remove noise from move calculations\n        self.alphazero.config.dirichlet_eps = 0\n\n    def select_action(self, state, search_iterations=200):\n        state_tensor = torch.tensor(self.alphazero.game.encode_state(state), dtype=torch.float).to(self.alphazero.config.device)\n        \n        # Get action without using search\n        if search_iterations == 0:\n            with torch.no_grad():\n                _, logits = self.alphazero.network(state_tensor.unsqueeze(0))\n\n            # Get action probs and mask for valid actions\n            action_probs = F.softmax(logits.view(-1), dim=0).cpu().numpy()\n            valid_actions = self.alphazero.game.get_valid_actions(state)\n            valid_action_probs = action_probs[valid_actions]\n            best_action = valid_actions[np.argmax(valid_action_probs)]\n            return best_action\n        # Else use MCTS \n        else:\n            action, _ = self.alphazero.mcts.search(state, search_iterations)\n            return action","metadata":{"execution":{"iopub.status.busy":"2023-09-14T23:19:55.887967Z","iopub.execute_input":"2023-09-14T23:19:55.888324Z","iopub.status.idle":"2023-09-14T23:19:55.895214Z","shell.execute_reply.started":"2023-09-14T23:19:55.888298Z","shell.execute_reply":"2023-09-14T23:19:55.894331Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### A Simple Game Playing Interface\nHere a human can play a game of Connect 4 with AlphaZero.","metadata":{}},{"cell_type":"code","source":"agent = AlphaZeroAgent(alphazero)\n\n# Set to 1 for AlphaZero to play first\nturn = 0\n\n# Reset the game\nstate = Connect4().reset()\ndone = False\n\n# Play loop\nwhile not done:\n    print(\"Current Board:\")\n    print(state)\n\n    if turn == 0:\n        print(\"Human to move.\")\n        action = int(input(\"Enter a move:\"))\n    else:\n        print(\"AlphaZero is thinking...\")\n        action = agent.select_action(state, 200)\n\n    next_state, reward, done = Connect4().step(state, action)\n\n    print(\"Board After Move:\")\n    print(next_state)\n\n    if done == True:\n        print(\"Game over\")\n    else:\n        state = -next_state\n        turn = 1 - turn","metadata":{"execution":{"iopub.status.busy":"2023-09-14T23:20:29.160753Z","iopub.execute_input":"2023-09-14T23:20:29.161087Z","iopub.status.idle":"2023-09-14T23:20:29.166326Z","shell.execute_reply.started":"2023-09-14T23:20:29.161061Z","shell.execute_reply":"2023-09-14T23:20:29.165178Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualization of AlphaZero vs a High Level AI\nHere we observe a Connect 4 game where our trained AlphaZero agent plays against a strong AI opponent that uses alpha-beta pruning for move selection. Alpha-beta pruning is an optimized version of the classic minimax tree search algorithm. \n\nThe game was recorded using Bryan Braun's Connect Four tool, accessible here: https://www.bryanbraun.com/connect-four/","metadata":{}},{"cell_type":"code","source":"def play(filename):\n    html = \"\"\n    video = open(filename,\"rb\").read()\n    src = \"data:video/mp4;base64,\" + b64encode(video).decode()\n    html += \"<video width=800 controls autoplay loop><source src='%s' type='video/mp4'></video>\" % src \n    return HTML(html)\n\nplay(\"/kaggle/input/alphazero-model-and-video/connect-4-recording.mp4\")","metadata":{"execution":{"iopub.status.busy":"2023-09-14T23:22:04.245368Z","iopub.execute_input":"2023-09-14T23:22:04.245711Z","iopub.status.idle":"2023-09-14T23:22:04.354271Z","shell.execute_reply.started":"2023-09-14T23:22:04.245683Z","shell.execute_reply":"2023-09-14T23:22:04.353263Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Finishing Remarks\nConstructing AlphaZero from the ground up has been both challenging and gratifying. The neural network components were relatively straightforward to assemble, but the Monte Carlo tree search was very fiddly.\n\nIn comparison to other AlphaZero implementations on GitHub, I hope for this to be among the more readable and easy to understand.\n\nFor anyone else also building AlphaZero from scratch, I hope this serves as a helpful reference.","metadata":{}}]}